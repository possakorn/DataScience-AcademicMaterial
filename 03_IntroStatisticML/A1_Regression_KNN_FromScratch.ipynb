{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14637119",
   "metadata": {},
   "source": [
    "# Assignment 1 Workbook\n",
    "\n",
    "This is the code template for your Assignment 1. You need to add code in the space provided for each question to get correct results. \n",
    "#### Do not modify any other code. Do not use any other libraries apart from those already in the code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c771af9",
   "metadata": {},
   "source": [
    "## Question 1a: Closed Form of Linear Regression derivation\n",
    "\n",
    "Linear regression can be expressed as $y_k=w_0\\times 1 + w_1x_{n1}+...+w_{m-1}x_{k,m-1}$, <br>\n",
    "where the training dataset $( {X}, {Y})$ has $n$ examples and $m$ features (where the first feature is 1).\n",
    "Equivalent matrix form is $\\hat{ {Y}}= {X} {W}$, where $X$ is a $n \\times m$ matrix, and $ {W}$ is a $m \\times 1$ vector of parameters and $\\hat{ {Y}}$ is the vector of predicted values y. Note that the first element of $W$ which is $w_0$ in an intercept.\n",
    "\n",
    "The cost function is $$J( {W}) = \\frac{1}{2n} \\sum\\limits_{i=0}^n(\\hat{y_i} - y_i)^2 \\tag{1}$$\n",
    "In the matrix form the cost function is $$J( {W})=\\frac{1}{2n}||\\hat{ {Y}} -  {Y}||_2^2=\\frac{1}{2n}|| {X} {W}- {Y}||_2^2 = \\frac{1}{2n}( {X} {W}- {Y})^T( {X} {W}- {Y})$$\n",
    "\n",
    "In order to find $ {W}$, we need to calculate the derivative of $J$ with respect to $ {W}$: $$\\frac{\\partial{J( {W})}}{\\partial{ {W}}}=\\nabla_w(J( {W}))$$ and compare it to zero. \n",
    "\n",
    "After the derivation you should obtain $ {W}=( {X}^T {X})^{-1} {X}^T {Y}$. \n",
    "For this derivation, you may need the following rules:<br> \n",
    "1. $ \\nabla_w(W^TAW)=2AW$ (if $A$ is symetrical), <br>\n",
    "2. $ W^TX^TY=(W^TX^TY)^T=Y^TXW$, if $W^TX^TY$ evaluates to a scalar.\n",
    "3. $ \\nabla_w(AW)=A^T $\n",
    "\n",
    "You tasks are as follows:\n",
    "1. Complete the derivation of $\\frac{\\partial{J( {W})}}{\\partial{ {W}}}$, then compare the derivative to 0 to obtain W in matrix form, given X and Y. \n",
    "2. Answer the questions in myUni Question 1a\n",
    "3. Code the derived formula and print sum W and cost J for the dataset given in the code using numpy and derived formula for W. Copy the results to myUni assignment Question 1b space.\n",
    "\n",
    "When using Python and numpy, be aware of the differences between multiply, matmul, @, np.dot and * functions and operators when applied to matrices.\n",
    "\n",
    "Do not use any other imports apart from ones already in this question's code. <br>\n",
    "\n",
    "\n",
    "======= your derivation can be done here. Or use other options specified in myUni for this question =================== <br>\n",
    "\n",
    "==================================================================================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ff2f611",
   "metadata": {},
   "source": [
    "## Question 1b: Closed Form of Linear Regression coding\n",
    "\n",
    "Complete the coding, run the code and enter the results in myUni Question 1b space. Put your code in the provided spaces. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "27925d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the folowing result to Question 1 \"(sumW = )\"\n",
      "8.21\n",
      "Please copy the folowing result to Question 1 \"(J = )\"\n",
      "3.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "Y = np.array([13, 8, 11, 2, 6])\n",
    "X = np.array([[1, 3, 5],[1, 6, 7],[1, 7, 8],[1, 8, 9],[1, 11, 12]])\n",
    "W = np.array([0, 0, 0])\n",
    "J = 0.0\n",
    "\n",
    "\"\"\"\n",
    "Use the above given Y, X and W to calculate final W and cost J \n",
    "using your derivation\n",
    "\"\"\"\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# NOTE: to print correctly, sumW and J must be scalar float\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "# calculate W following the equation from question 1a\n",
    "W =  np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), Y)\n",
    "\n",
    "# calculate Cost function - J following \n",
    "J_a = np.matmul(X, W) - Y\n",
    "J = np.matmul(J_a.T, J_a) / (len(X) * 2)\n",
    "\n",
    "#============================================================\n",
    "\n",
    "print('Please copy the folowing result to Question 1 \"(sumW = )\"')\n",
    "print(np.round(np.sum(W),2))\n",
    "print('Please copy the folowing result to Question 1 \"(J = )\"')\n",
    "print(np.round(J,2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15e00ff6",
   "metadata": {},
   "source": [
    "## Question 2: Linear Regression with Gradient Descent\n",
    "\n",
    "Implement linear regression with gradient descent using the below code template. Put your code in provided spaces to obtain correct results. <br>\n",
    "\n",
    "Use the same formula of the Linear Regression cost as in Question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "3f3795cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "dataset = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "# selecting features\n",
    "X_train = np.array(dataset[[\"age\", \"sex\", \"bmi\", \"bp\"]])\n",
    "\n",
    "# normalising numerical features\n",
    "X_train = (np.max(X_train,axis=0)-X_train)/(np.max(X_train,axis=0)-np.min(X_train,axis=0))\n",
    "\n",
    "# adding '1' column for the intercept\n",
    "X_train = np.concatenate((np.ones(X_train.shape[0]).reshape(X_train.shape[0],1), X_train), axis=1)\n",
    "\n",
    "# forming target\n",
    "Y_train = np.array([dataset[\"target\"]])\n",
    "Y_train = Y_train.reshape(X_train.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8f2207a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_compute_cost(X, Y, W): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "      X (ndarray (n,m)): Data, n examples with m features\n",
    "      y (ndarray (n,1)) : target values\n",
    "      w (ndarray (m,1)) : model parameters  \n",
    "      \n",
    "    Returns:\n",
    "      J (scalar): cost\n",
    "    \"\"\"\n",
    "    J = 0\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "    # calculate number of instance\n",
    "    m = len(Y)\n",
    "    \n",
    "    # calculate Hypothesis\n",
    "    h0 = np.dot(X, W)\n",
    "    \n",
    "    # construct the cost function formula\n",
    "    J = np.sum((h0 - Y)**2 / (m * 2))\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "eb6b8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_compute_gradient(X, Y, W): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X : Data,n examples with n features\n",
    "      Y : n target values\n",
    "      W : m model parameters \n",
    "      \n",
    "    Returns:\n",
    "      dJ_dW : The gradient of the cost w.r.t. the parameters W, m values. \n",
    "    \"\"\"\n",
    "    dJ_dW = 0\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "    # calculate number of instance\n",
    "    m = len(Y)\n",
    "    \n",
    "    # calculate Hypothesis\n",
    "    h0 = np.dot(X, W)\n",
    "\n",
    "    # construct the gradient of the cost\n",
    "    dJ_dW = X.T.dot(h0 - Y) / m\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "    return dJ_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "af21cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_gradient_descent(X, Y, W_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X                   : Data, n examples with m features\n",
    "      Y                   : n target values\n",
    "      W_in                : m initial model parameters  \n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      W                   : final m values of parameters \n",
    "      J (scalar)          : final cost\n",
    "      \"\"\"\n",
    "    W = 0\n",
    "    J = 0\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "    \n",
    "    # Apply initial model parameters with input\n",
    "    W = W_in\n",
    "    \n",
    "    # Loop with setting number of iterations to optimize the gradient \n",
    "    for i in range(num_iters):\n",
    "      W = W - alpha * gradient_function(X, Y, W)\n",
    "\n",
    "    J = cost_function(X, Y, W)\n",
    "    \n",
    "# ===========================================================\n",
    "\n",
    "    return W, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "a7b86bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the folowing result line to Question 2 \"(sumW = )\"\n",
      "72.02\n",
      "Please copy the folowing result line to Question 3 \"(J = )\"\n",
      "1923.46\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_W = np.ones(X_train.shape[1]).reshape(5,1)\n",
    "\n",
    "# gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 0.05\n",
    "\n",
    "\"\"\"\n",
    "Apply functions coded above to calculate final W and cost J after training\n",
    "Use given datasets and parameters\n",
    "\"\"\"\n",
    "W = 0\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "# NOTE: to print correctly, sumW and J must be scalar float\n",
    "\n",
    "W, J = linreg_gradient_descent(X_train, \n",
    "                                Y_train, \n",
    "                                initial_W, \n",
    "                                linreg_compute_cost, \n",
    "                                linreg_compute_gradient, \n",
    "                                alpha, \n",
    "                                iterations\n",
    "                                )\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print('Please copy the folowing result line to Question 2 \"(sumW = )\"')\n",
    "print(np.round(np.sum(W),2))\n",
    "print('Please copy the folowing result line to Question 3 \"(J = )\"')\n",
    "print(np.round(J,2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ff078dd",
   "metadata": {},
   "source": [
    "## Question 3: Logistic Regression with Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f6b8d90",
   "metadata": {},
   "source": [
    "Complete the code where indicated implementing logistic legression and gradient descent. Recall that the cost function in logistic regression is\n",
    "\n",
    "$$ J(W) = \\frac{1}{n} \\left[ -Y \\log\\left(h\\left( X \\right) \\right) - \\left( 1 - Y\\right) \\log \\left( 1 - h\\left( X \\right) \\right) \\right]$$\n",
    "\n",
    "n is number of instances. and the gradient of the cost is a vector of the same length as $W$ (where $W$ includes $w_0$) defined as follows:\n",
    "\n",
    "$$ \\frac{\\partial J(W)}{\\partial W} =\\nabla_W = \\frac{1}{n} \\left( h \\left( X \\right) - Y \\right) X $$\n",
    "\n",
    "Note that while this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of hypothesis function $h(X)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "f8aefaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "data = df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch']].dropna()\n",
    "data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 1\n",
    "data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 0\n",
    "data = np.array(data)\n",
    "X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "# normalise all columns of X using min-max\n",
    "for c in range(X.shape[1]):\n",
    "    X[:,c] = (max(X[:,c]) -  X[:,c])/(max(X[:,c]) - min(X[:,c]))\n",
    "    \n",
    "# break into train/test, with 80% training and 20% test\n",
    "split = int(0.8 * data.shape[0])\n",
    "\n",
    "X_train = X[:split]\n",
    "X_test = X[split:]\n",
    "Y_train = Y[:split]\n",
    "Y_test = Y[split:]\n",
    "\n",
    "# Add intercept term to X_train and X_test\n",
    "X_train = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=1)\n",
    "X_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)\n",
    "\n",
    "# ================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "0f8bc8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid function given the input z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    g : array_like\n",
    "        The computed sigmoid function. g has the same shape as z, since\n",
    "        the sigmoid is computed element-wise on z.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the sigmoid of each value of z (z can be a matrix, vector or scalar).\n",
    "    \"\"\"\n",
    "    # convert input to a numpy array\n",
    "    z = np.array(z).astype(\"float\")\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================  \n",
    "    # DO NOT use any other import statements for this question\n",
    "    \n",
    "    # construct the sigmoid function\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "694a0855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_costFunction(W, X, Y):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W : m parameters for logistic regression. \n",
    "    \n",
    "    X : The input dataset of shape (n,m) where n is the total number\n",
    "        of data points and m is the number of features. We assume the \n",
    "        intercept has already been added to the input.\n",
    "    \n",
    "    Y : Vector of labels for the input with n elements. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function. \n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    n = Y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # DO NOT use any other import statements for this question\n",
    "    # NOTE: the diff between dot and @ is that dot allow nultiply matrix by scalar, @ does not\n",
    "    \n",
    "    # calculate Hypothesis\n",
    "    h0 = sigmoid(X.dot(W))\n",
    "    \n",
    "    # construct the gradient of the cost\n",
    "    J = (-1 / n) * (Y.dot(np.log(h0)) + (1 - Y).dot(np.log(1 - h0)))\n",
    "    \n",
    "\n",
    "    # =============================================================\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "a7de3b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_GradFunction(W, X, Y):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W : m parameters for logistic regression. \n",
    "    \n",
    "    X : The input dataset of shape (n,m) where n is the total number\n",
    "        of data points and m is the number of features. We assume the \n",
    "        intercept has already been added to the input.\n",
    "    \n",
    "    Y : Vector of labels for the input with n elements. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad : A vector with m values which is the gradient of the cost\n",
    "        function with respect to W, at the current values of W.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    n = Y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    grad = np.zeros(W.shape)\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # DO NOT use any other import statements for this question\n",
    "    # NOTE: the diff between dot and @ is that dot allow nultiply matrix by scalar, @ does not\n",
    "\n",
    "    # calculate Hypothesis\n",
    "    h0 = sigmoid(np.dot(X, W))\n",
    "    \n",
    "    # construct the gradient of the cost\n",
    "    grad = X.T.dot(h0 - Y) / n\n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "a9ecf20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_gradient_descent(X, Y, W_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn W. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X                   : Data, n examples with m features\n",
    "      Y                   : m target values\n",
    "      W_in                : m initial model parameters  \n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      W                : Updated values of parameters \n",
    "      \"\"\"\n",
    "    W = W_in\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # DO NOT use any other import statements for this question\n",
    "    \n",
    "    # Loop with setting number of iterations to optimize the gradient \n",
    "    for i in range(num_iters):\n",
    "      W = W - alpha * gradient_function(W, X, Y)\n",
    "      \n",
    "    J = cost_function(W, X, Y)\n",
    "\n",
    "    # =============================================================\n",
    "      \n",
    "    return J, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "616acc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the folowing result line to Question 3 \"(sumW = )\"\n",
      "0.4\n",
      "Please copy the folowing result line to Question 3 \"(J = )\"\n",
      "0.56\n",
      "Please copy the folowing result line to Question 3 \"(Accuracy = )\"\n",
      "0.78\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize parameters\n",
    "initial_W = np.array([-40.0]*X_train.shape[1])\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 20000\n",
    "alpha = 0.02\n",
    "\n",
    "W = 0\n",
    "J = 0\n",
    "acc = 0\n",
    "\n",
    "\"\"\"\n",
    "Apply functions coded above to calculate:\n",
    "    final W after training,\n",
    "    cost J for training set after training\n",
    "    accuracy for test set\n",
    "Use given datasets and parameters\n",
    "\"\"\"\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# NOTE: to print correctly, W must be of shape (5,), J must be scalar float\n",
    "\n",
    "J, W = logreg_gradient_descent(X_train, \n",
    "                               Y_train, \n",
    "                               initial_W, \n",
    "                               logreg_costFunction, \n",
    "                               logreg_GradFunction, \n",
    "                               alpha, \n",
    "                               iterations\n",
    "                               )\n",
    "\n",
    "def cal_accuracy(y_true, y_pred):\n",
    "    acc = np.sum(np.equal(y_true, y_pred)) / len(y_true)\n",
    "    return acc\n",
    "\n",
    "y_pred = np.where(X_test.dot(W) >= 0,1,0)                               \n",
    "acc = cal_accuracy(Y_test, y_pred)\n",
    "\n",
    "# ===========================================================\n",
    "\n",
    "print('Please copy the folowing result line to Question 3 \"(sumW = )\"')\n",
    "print(np.round(np.sum(W), 2))\n",
    "print('Please copy the folowing result line to Question 3 \"(J = )\"')\n",
    "print(np.round(J,2))\n",
    "print('Please copy the folowing result line to Question 3 \"(Accuracy = )\"')\n",
    "print(np.round(acc,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "687fc6a0",
   "metadata": {},
   "source": [
    "## Question 4: Regularising Logistic Regression\n",
    "\n",
    "Now you will implement regularized logistic regression. Complete the code where indicated.\n",
    "\n",
    "Recall that the regularized cost function in logistic regression is\n",
    "\n",
    "$$ J(W) = \\frac{1}{n} \\left[ -Y \\log\\left(h\\left( X \\right) \\right) - \\left( 1 - Y\\right) \\log \\left( 1 - h\\left( X \\right) \\right) \\right] + \\frac{\\lambda}{2n} \\sum_{j=1}^m w_j^2 \\qquad \\text{ for } j > 0$$\n",
    "\n",
    "Note that you should not regularize the parameters $w_0$. The gradient of the cost function is a vector defined as follows:\n",
    "\n",
    "$$ \\frac{\\partial J(W)}{\\partial w_0} =\\nabla_{w_0} = \\frac{1}{n} \\left( h \\left( X \\right) - Y \\right) X_0 \\qquad \\text{ where } X_0 \\text{ is a vector or 1's, corresponding to intercept } w_0 $$\n",
    "\n",
    "$$ \\frac{\\partial J(W)}{\\partial w_j} =\\nabla_{w_j} = \\frac{1}{n} \\left( h \\left( X \\right) - Y \\right) X_j  + \\frac{\\lambda}{n}w_j \\qquad \\text{ where } X_j \\text{ is the array of X's, except } X_0 \\text{ corresponding to intercept } w_0 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "7f8aaa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "data = df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch']].dropna()\n",
    "data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 1\n",
    "data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 0\n",
    "data = np.array(data)\n",
    "X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "# normalise all cols\n",
    "for c in range(X.shape[1]):\n",
    "    X[:,c] = (max(X[:,c]) -  X[:,c])/(max(X[:,c]) - min(X[:,c]))\n",
    "    \n",
    "# break into train/test\n",
    "split = int(0.8 * data.shape[0])\n",
    "\n",
    "X_train = X[:split]\n",
    "X_test = X[split:]\n",
    "Y_train = Y[:split]\n",
    "Y_test = Y[split:]\n",
    "\n",
    "# Add intercept term to X\n",
    "X_train = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=1)\n",
    "X_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "88ae30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_costFunctionReg(W, X, Y, lambda_):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression with regularization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W : Logistic regression vector of m parameters,\n",
    "        where m is the number of features including any intercept.\n",
    "    \n",
    "    X : The data set with shape (n,m). n is the number of examples, and\n",
    "        m is the number of features.\n",
    "    \n",
    "    y : The vector of data labels of size n.\n",
    "    \n",
    "    lambda_ : float\n",
    "        The regularization parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the regularized cost function. \n",
    "   \n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    n = Y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "\n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "    # DO NOT use any other import statements for this question\n",
    "    \n",
    "    # calculate Hypothesis\n",
    "    h0 = sigmoid(X.dot(W))\n",
    "\n",
    "    # adding regularization terms \n",
    "    L2_reg_cost = (lambda_ / (2 * n)) * np.sum(W[1:]**2) # for j > 1\n",
    "\n",
    "    \n",
    "    # construct the gradient of the cost\n",
    "    J = (-1 / n) * (Y.dot(np.log(h0)) + (1 - Y).dot(np.log(1 - h0))) + L2_reg_cost\n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "fea96064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_GradFunctionReg(W, X, Y, lambda_):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression with regularization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W : Logistic regression vector of m parameters,\n",
    "        where m is the number of features including any intercept.\n",
    "    \n",
    "    X : The data set with shape (n,m). n is the number of examples, and\n",
    "        m is the number of features.\n",
    "    \n",
    "    y : The vector of data labels of size n.\n",
    "    \n",
    "    lambda_ : float\n",
    "        The regularization parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad : A vector of size m which is the gradient of the cost\n",
    "        function with respect to theta, at the current values of theta.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    n = Y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    grad = np.zeros(W.shape)\n",
    "\n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "    # DO NOT use any other import statements for this question\n",
    "    \n",
    "    # calculate Hypothesis\n",
    "    h0 = sigmoid(np.dot(X, W))\n",
    "    \n",
    "    # adding regularization terms\n",
    "    L2_reg_grad = lambda_ * W / n\n",
    "    L2_reg_grad[0] = 0 # no regularization terms on intercept\n",
    "\n",
    "    # construct the gradient of the cost\n",
    "    grad = ( X.T.dot(h0 - Y) / n ) + L2_reg_grad\n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "39890595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_gradient_descent_reg(X, Y, W, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X                   : Data ndarray array, n examples with m features\n",
    "      Y                   : ndarray vector of target n values\n",
    "      W                   : ndarray vector of initial m model parameters \n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      W (ndarray)         : Updated values of parameters \n",
    "      \"\"\"\n",
    "    \n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "    # DO NOT use any other import statements for this question\n",
    "        \n",
    "    # Loop with setting number of iterations to optimize the gradient \n",
    "    for i in range(num_iters):\n",
    "      if i > 0:\n",
    "        W = W - alpha * gradient_function(W, X, Y, lambda_)\n",
    "\n",
    "    J = cost_function(W, X, Y, lambda_)\n",
    "\n",
    "    # =============================================================\n",
    "    \n",
    "    return J, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "ef227feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the folowing result line to Question 4 \"(sumW = )\"\n",
      "1.95\n",
      "Please copy the folowing result line to Question 4 \"(J = )\"\n",
      "0.49\n",
      "Please copy the folowing result line to Question 4 \"(Accuracy = )\"\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize parameters\n",
    "initial_W = np.array([-40]*X_train.shape[1])\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 20000\n",
    "alpha = 0.02\n",
    "lambda_ = 2\n",
    "W = 0\n",
    "J = 0\n",
    "acc = 0\n",
    "\n",
    "\"\"\"\n",
    "Apply functions coded above to calculate:\n",
    "    final W after training,\n",
    "    cost J for training set after training\n",
    "    accuracy for test set\n",
    "Use given datasets and parameters\n",
    "\"\"\"\n",
    "\n",
    "# ===================== YOUR CODE HERE ======================\n",
    "# DO NOT use any other import statements for this question\n",
    "# NOTE: to print correctly, sumW and J must be scalar float\n",
    "\n",
    "J, W = logreg_gradient_descent_reg(X_train, \n",
    "                                   Y_train, \n",
    "                                   initial_W, \n",
    "                                   logreg_costFunctionReg, \n",
    "                                   logreg_GradFunctionReg, \n",
    "                                   alpha, \n",
    "                                   iterations, \n",
    "                                   lambda_\n",
    "                                   )\n",
    "\n",
    "y_pred = np.where(X_test.dot(W) >= 0,1,0) \n",
    "acc = cal_accuracy(Y_test, y_pred)\n",
    "\n",
    "# ===========================================================\n",
    "\n",
    "print('Please copy the folowing result line to Question 4 \"(sumW = )\"')\n",
    "print(np.round(np.sum(W), 2))\n",
    "print('Please copy the folowing result line to Question 4 \"(J = )\"')\n",
    "print(np.round(J,2))\n",
    "print('Please copy the folowing result line to Question 4 \"(Accuracy = )\"')\n",
    "print(np.round(acc,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c2ac2ec",
   "metadata": {},
   "source": [
    "## Queation 5: Implementing k-NN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "6868c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "data = df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch']].dropna()\n",
    "data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 1\n",
    "data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 0\n",
    "data = np.array(data)\n",
    "X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "# normalise all cols\n",
    "for c in range(X.shape[1]):\n",
    "    X[:,c] = (max(X[:,c]) -  X[:,c])/(max(X[:,c]) - min(X[:,c]))\n",
    "    \n",
    "# break into train/test\n",
    "split = int(0.8 * data.shape[0])\n",
    "\n",
    "X_train = X[:split]\n",
    "X_test = X[split:]\n",
    "Y_train = Y[:split]\n",
    "Y_test = Y[split:]\n",
    "\n",
    "# Add intercept term to X\n",
    "X_train = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=1)\n",
    "X_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "376cf2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2):\n",
    "    \"\"\"\n",
    "    Calculates Euclidean distance between two point x1 and x2.\n",
    "    \n",
    "    Args:\n",
    "      x1                  : Data point, a numeric vector of size n (number of features)\n",
    "      x2                  : Data point, a numeric vector of size n (number of features)\n",
    "      \n",
    "    Returns:\n",
    "      d (float)           : Euclidean distance (scalar) between x1 and x2 in feature space \n",
    "      \"\"\"\n",
    "    \n",
    "    d = 0.0\n",
    "    \n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "    # DO NOT use any other import statements for this question\n",
    "    \n",
    "    # construct Euclidean distance\n",
    "    d = np.sqrt(np.sum((x1 - x2)**2))\n",
    "\n",
    "    # ===========================================================\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "982e2d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, Y_train, X_test, k=9):\n",
    "    \"\"\"\n",
    "    Trains the k nearest neighbour model using X_train, Y_train dataset, \n",
    "    then tests the model on X_test, and returns predictions for the test set.\n",
    "    \n",
    "    Args:\n",
    "      X_train  : Training data, ndarray array, n examples with m features\n",
    "      Y_train  : ndarray vector of target n values\n",
    "      X_test   : Test data, ndarray array, n_test examples with m features\n",
    "      k (int)  : number of neighbour points to be used by k-NN\n",
    "      \n",
    "    Returns:\n",
    "      Y_pred   : ndarray vector of predicted n values, one predicted value for each test example\n",
    "      \"\"\"\n",
    "    Y_pred = []\n",
    "\n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "    # DO NOT use any other import statements for this question\n",
    "    \n",
    "    for i in X_test:\n",
    "      neighbors = []\n",
    "      neighbors_labels = []\n",
    "      \n",
    "      for j in X_train:\n",
    "        distances = distance(i, j)\n",
    "        neighbors.append(distances)\n",
    "        \n",
    "      # change to array\n",
    "      neighbors = np.array(neighbors)\n",
    "      \n",
    "      # get distance, index, and labels for first k neighbors\n",
    "      neighbors_sort = np.sort(neighbors)[:k]\n",
    "      neighbors_sort_index = np.argsort(neighbors)[:k]\n",
    "      neighbors_labels = Y_train[neighbors_sort_index]\n",
    "      \n",
    "      # find the most common by select the highest count from first k neighbors\n",
    "      labels, labels_index = np.unique(neighbors_labels, return_counts = True)\n",
    "      k_nearest_labels = labels[np.argmax(labels_index)]\n",
    "      \n",
    "      # append the result to the output Y prediction vector\n",
    "      Y_pred.append(k_nearest_labels)\n",
    "    \n",
    "    # ============================================================= \n",
    "        \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "29f0933d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the folowing result line to Question 5 \"(Accuracy = )\"\n",
      "0.83\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Apply knn function coded above to calculate the accuracy\n",
    "Use already read dataset and k = 9\n",
    "\"\"\"\n",
    "accuracy = 0\n",
    "\n",
    "# ===================== YOUR CODE HERE ======================\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "Y_pred = knn(X_train, \n",
    "             Y_train, \n",
    "             X_test, \n",
    "             k=9\n",
    "             )\n",
    "\n",
    "accuracy = cal_accuracy(Y_test, Y_pred)\n",
    "\n",
    "# ===========================================================\n",
    "\n",
    "print('Please copy the folowing result line to Question 5 \"(Accuracy = )\"')\n",
    "print(np.round(accuracy,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
