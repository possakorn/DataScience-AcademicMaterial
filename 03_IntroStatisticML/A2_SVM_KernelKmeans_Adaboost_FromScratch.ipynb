{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50036eab",
   "metadata": {},
   "source": [
    "# Assignment 2 Workbook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edc1ead5",
   "metadata": {},
   "source": [
    "## Question 1: SVM Primal Form\n",
    "\n",
    "Please implement the training and testing algorithms of soft-margin Linear Support Vector Machine in its primal form, that is,\n",
    "\n",
    "$$\\min_{\\mathbf{w},b,\\{\\xi_i\\}} \\frac{1}{2} \\|\\mathbf{w}\\|_2^2 + \\frac{C}{N} \\sum_{i=1}^N \\xi_i \\nonumber \\\\ s.t.~~ y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1 - \\xi_i, ~~\\forall i \\nonumber \\\\ \\xi_i \\ge 0 \\nonumber$$\n",
    "Use CVXPY in your implementation strictly following the format given in this Notebook, and supplying missing code in the indicated space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14fe37d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtfdriY9Fhyw",
    "outputId": "adc09825-1942-4209-ab34-e4d6148a3d8e"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4d7723",
   "metadata": {
    "id": "huRMLmS2IQjT"
   },
   "outputs": [],
   "source": [
    "# get training dataset\n",
    "train = \"train.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X_train = df[:2000].iloc[:, 1:].to_numpy()\n",
    "Y_train = df[:2000].iloc[:, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9af991c",
   "metadata": {
    "id": "4G3CJIcaIogN"
   },
   "outputs": [],
   "source": [
    "# get test dataset\n",
    "test = \"test.csv\"\n",
    "df = pd.read_csv(test, header=None)\n",
    "X_test = df.iloc[:1000, 1:].to_numpy()\n",
    "Y_test = df.iloc[:1000, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "282b9284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape (2000, 200)\n",
      "y train shape (2000,)\n",
      "X test shape (1000, 200)\n",
      "y test shape (1000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train shape\",X_train.shape)\n",
    "print(\"y train shape\",Y_train.shape)\n",
    "print(\"X test shape\",X_test.shape)\n",
    "print(\"y test shape\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a355188",
   "metadata": {
    "id": "QhiZQBI0Fhy3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status: optimal\n",
      "optimal value 4.424257612089827\n",
      "optimal var [[-0.36 -0.91 -0.99 ...  0.3   2.44 -1.26]\n",
      " [-1.4  -1.9   0.09 ... -0.2  -0.92 -0.46]\n",
      " [-0.43  1.45 -0.68 ...  0.12  0.01 -0.56]\n",
      " ...\n",
      " [ 1.06  1.93 -0.17 ...  1.17 -0.4  -0.97]\n",
      " [-1.02 -0.81  1.01 ... -0.97 -0.34 -0.8 ]\n",
      " [-0.75 -1.09 -0.14 ...  1.31 -0.01  1.15]] [-1.  1.  1. ... -1. -1.  1.]\n",
      "Please copy the folowing result to Question 1 \"sum(W) = \n",
      "0.79\n",
      "Please copy the folowing result to Question 1 \"b = \"\n",
      "1.75\n"
     ]
    }
   ],
   "source": [
    "# train linear svm in primal form\n",
    "def svm_train_primal(data_train, label_train, regularisation_para_c):\n",
    "    X, Y = data_train, label_train\n",
    "    n_samples, m_features = np.shape(X)\n",
    "    \n",
    "    W_value = 0\n",
    "    b_value = 0\n",
    "    slack_var_value = 0\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "# ================================================================\n",
    "    \n",
    "    # setting the parameter\n",
    "    W = cp.Variable(m_features)\n",
    "    b = cp.Variable()\n",
    "    slack_var = cp.Variable(n_samples)\n",
    "    \n",
    "    # objective: Hinge loss functions\n",
    "    obj = cp.Minimize(\n",
    "        0.5 * cp.norm(W, 2)**2 + regularisation_para_c / n_samples * cp.sum(slack_var)\n",
    "    )\n",
    "    \n",
    "    # constraints: SVM primal form\n",
    "    cons1 = [Y[i] * (W.T @ X[i] + b) >= 1 - slack_var[i] for i in range(n_samples)]\n",
    "    cons2 = [slack_var >= 0]\n",
    "    cons =  cons1 + cons2\n",
    "    \n",
    "    # Form and solve problem\n",
    "    prob = cp.Problem(obj, cons)\n",
    "    prob.solve() # Returns the optimal value\n",
    "    \n",
    "    # print the problem info - remove it after\n",
    "    print(\"status:\", prob.status)\n",
    "    print(\"optimal value\", prob.value)\n",
    "    print(\"optimal var\", X, Y)\n",
    "    \n",
    "    # Extract the values - a numpy ndarray\n",
    "    W_value = W.value\n",
    "    b_value = b.value\n",
    "    slack_var_value = slack_var.value\n",
    "\n",
    "    return [W_value, b_value, slack_var_value]\n",
    "\n",
    "# train primal model\n",
    "c = 100\n",
    "model_primal = svm_train_primal(X_train, Y_train, c)\n",
    "\n",
    "# output svm primal form solutions\n",
    "W = model_primal[0]\n",
    "b = model_primal[1]\n",
    "\n",
    "print('Please copy the folowing result to Question 1 \"sum(W) = ')\n",
    "print(np.round(np.sum(W),2))\n",
    "print('Please copy the folowing result to Question 1 \"b = \"')\n",
    "print(np.round(b,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fde72cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the folowing result line to Question 1 \"Accuracy = )\"\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "# predict accuracy of svm model on test dataset\n",
    "def svm_predict(data_test, label_test, svm_model):\n",
    "    \n",
    "    acc = 0.0\n",
    "    \n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "# ==========================================================\n",
    "    # Convert test data and labels to NumPy arrays\n",
    "    X_test = np.array(data_test)\n",
    "    Y_test = np.array(label_test)\n",
    "    \n",
    "    # Retrieve the model parameters\n",
    "    W_value = svm_model[0]\n",
    "    b_value = svm_model[1]\n",
    "    \n",
    "    # Compute the predictions\n",
    "    pred = np.sign(X_test @ W_value + b_value)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    acc= np.mean(pred == Y_test)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# predict and output primal accuracy\n",
    "accuracy = svm_predict(X_test, Y_test, model_primal)\n",
    "print('Please copy the folowing result line to Question 1 \"Accuracy = )\"')\n",
    "print(np.round(accuracy,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "130676a2",
   "metadata": {},
   "source": [
    "## Question 2: SVM Dual Form\n",
    "\n",
    "Please implement the training and testing algorithms of soft-margin Linear Support Vector Machine in its **dual** form, that is,\n",
    "\n",
    "$$\\max_{\\alpha_i}\\sum_i \\alpha_i - \\frac{1}{2}\\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j <\\mathbf{x}_i, \\mathbf{x}_j> \\nonumber \\\\ s.t. ~~~ 0 \\le \\alpha_i \\le \\frac{C}{N} \\nonumber \\\\ ~~~ \\sum_i \\alpha_i y_i = 0 \\nonumber$$\n",
    "\n",
    "Use CVXPY in your implementation strictly following the format given in this Notebook, and supplying missing code in the indicated space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944c723a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtfdriY9Fhyw",
    "outputId": "adc09825-1942-4209-ab34-e4d6148a3d8e"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9760d985",
   "metadata": {
    "id": "huRMLmS2IQjT"
   },
   "outputs": [],
   "source": [
    "# get training dataset\n",
    "train = \"train.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X_train = df[:2000].iloc[:, 1:].to_numpy()\n",
    "Y_train = df[:2000].iloc[:, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8287b42",
   "metadata": {
    "id": "4G3CJIcaIogN"
   },
   "outputs": [],
   "source": [
    "# get test dataset\n",
    "test = \"test.csv\"\n",
    "df = pd.read_csv(test, header=None)\n",
    "X_test = df.iloc[:1000, 1:].to_numpy()\n",
    "Y_test = df.iloc[:1000, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5898843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (2000, 200)\n",
      "Y.shape (2000,)\n",
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.3.2                                    \n",
      "===============================================================================\n",
      "(CVXPY) Aug 04 01:00:01 AM: Your problem has 2000 variables, 3 constraints, and 0 parameters.\n",
      "(CVXPY) Aug 04 01:00:01 AM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Aug 04 01:00:01 AM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Aug 04 01:00:01 AM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Aug 04 01:00:01 AM: Compiling problem (target solver=SCS).\n",
      "(CVXPY) Aug 04 01:00:01 AM: Reduction chain: FlipObjective -> Dcp2Cone -> CvxAttr2Constr -> ConeMatrixStuffing -> SCS\n",
      "(CVXPY) Aug 04 01:00:01 AM: Applying reduction FlipObjective\n",
      "(CVXPY) Aug 04 01:00:01 AM: Applying reduction Dcp2Cone\n",
      "(CVXPY) Aug 04 01:00:01 AM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Aug 04 01:00:01 AM: Applying reduction ConeMatrixStuffing\n",
      "(CVXPY) Aug 04 01:00:01 AM: Applying reduction SCS\n",
      "(CVXPY) Aug 04 01:00:01 AM: Finished problem compilation (took 3.730e-01 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Aug 04 01:00:01 AM: Invoking solver SCS  to obtain a solution.\n",
      "------------------------------------------------------------------\n",
      "\t       SCS v3.2.3 - Splitting Conic Solver\n",
      "\t(c) Brendan O'Donoghue, Stanford University, 2012\n",
      "------------------------------------------------------------------\n",
      "problem:  variables n: 2200, constraints m: 4201\n",
      "cones: \t  z: primal zero / dual free vars: 201\n",
      "\t  l: linear vars: 4000\n",
      "settings: eps_abs: 1.0e-05, eps_rel: 1.0e-05, eps_infeas: 1.0e-07\n",
      "\t  alpha: 1.50, scale: 1.00e-01, adaptive_scale: 1\n",
      "\t  max_iters: 100000, normalize: 1, rho_x: 1.00e-06\n",
      "\t  acceleration_lookback: 10, acceleration_interval: 10\n",
      "lin-sys:  sparse-direct-amd-qdldl\n",
      "\t  nnz(A): 404689, nnz(P): 200\n",
      "------------------------------------------------------------------\n",
      " iter | pri res | dua res |   gap   |   obj   |  scale  | time (s)\n",
      "------------------------------------------------------------------\n",
      "     0| 7.61e+00  2.06e+00  1.03e+03 -5.36e+02  1.00e-01  4.35e-01 \n",
      "   250| 3.99e-03  9.18e-03  7.24e-03 -4.86e+00  4.18e-01  1.35e+00 \n",
      "   500| 1.71e-03  7.67e-03  1.02e-03 -4.51e+00  4.18e-01  1.83e+00 \n",
      "   750| 7.90e-04  2.39e-03  4.24e-04 -4.45e+00  4.18e-01  2.24e+00 \n",
      "  1000| 4.38e-04  9.43e-04  2.28e-04 -4.44e+00  4.18e-01  2.64e+00 \n",
      "  1250| 2.99e-04  7.56e-04  3.23e-05 -4.43e+00  4.18e-01  3.04e+00 \n",
      "  1500| 2.31e-04  5.92e-04  1.50e-06 -4.43e+00  4.18e-01  3.45e+00 \n",
      "  1750| 1.10e-02  1.68e-02  4.64e-04 -4.43e+00  4.18e-01  3.82e+00 \n",
      "  2000| 1.14e-04  2.74e-04  3.88e-05 -4.43e+00  4.18e-01  4.24e+00 \n",
      "  2250| 9.88e-05  2.09e-04  2.80e-05 -4.43e+00  4.18e-01  4.64e+00 \n",
      "  2500| 7.28e-05  1.36e-04  1.51e-06 -4.42e+00  4.18e-01  5.07e+00 \n",
      "  2750| 5.05e-05  9.80e-05  7.04e-06 -4.42e+00  4.18e-01  5.49e+00 \n",
      "  3000| 4.32e-05  1.33e-04  7.93e-06 -4.42e+00  4.18e-01  5.90e+00 \n",
      "  3250| 3.46e-05  5.25e-05  7.52e-06 -4.42e+00  4.18e-01  6.30e+00 \n",
      "  3500| 2.51e-05  3.29e-05  4.90e-06 -4.42e+00  4.18e-01  6.72e+00 \n",
      "  3750| 1.94e-05  2.13e-05  4.87e-06 -4.42e+00  4.18e-01  7.13e+00 \n",
      "  4000| 1.57e-05  2.12e-05  2.49e-06 -4.42e+00  4.18e-01  7.56e+00 \n",
      "  4250| 1.48e-05  1.45e-05  1.27e-07 -4.42e+00  4.18e-01  7.96e+00 \n",
      "  4500| 7.53e-06  2.40e-05  3.90e-06 -4.42e+00  1.32e+00  8.51e+00 \n",
      "  4550| 7.08e-06  1.75e-05  3.23e-06 -4.42e+00  1.32e+00  8.59e+00 \n",
      "------------------------------------------------------------------\n",
      "status:  solved\n",
      "timings: total: 8.59e+00s = setup: 4.29e-01s + solve: 8.16e+00s\n",
      "\t lin-sys: 7.13e+00s, cones: 9.24e-02s, accel: 4.49e-02s\n",
      "------------------------------------------------------------------\n",
      "objective = -4.424262\n",
      "------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "                                    Summary                                    \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Aug 04 01:00:10 AM: Problem status: optimal\n",
      "(CVXPY) Aug 04 01:00:10 AM: Optimal value: 4.424e+00\n",
      "(CVXPY) Aug 04 01:00:10 AM: Compilation took 3.730e-01 seconds\n",
      "(CVXPY) Aug 04 01:00:10 AM: Solver (including time spent in interface) took 8.593e+00 seconds\n",
      "[ 4.99982310e-02 -7.24912060e-08  5.07187294e-07 ...  2.81612253e-07\n",
      " -1.37782723e-06  6.49550290e-07]\n",
      "Please copy the folowing result to Question 2 \"Sum of alphas = )\"\n",
      "5.78\n"
     ]
    }
   ],
   "source": [
    "# train linear svm in dual form\n",
    "def svm_train_dual(data_train, label_train, regularisation_para_c):\n",
    "    \n",
    "    alphas = 0\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "    \n",
    "# ===========================================================\n",
    "\n",
    "    # setting the parameter\n",
    "    X, Y = data_train, label_train\n",
    "    # Y = label_train\n",
    "    n_samples, m_features = X.shape\n",
    "    \n",
    "    alphas = cp.Variable(n_samples)\n",
    "    # Y = Y.reshape(-1)\n",
    "    \n",
    "    # objective: dual  matrix \n",
    "    print(\"X.shape\", X.shape)\n",
    "    print(\"Y.shape\", Y.shape)\n",
    "    # Gram matrix\n",
    "    # K = X @ X.T\n",
    "    # Q matrix\n",
    "    # Q = Y @ Y.T * K\n",
    "    # P = cp.matmul(Y, Y.T) @ cp.matmul(X, X.T)\n",
    "    # results = alphas.T @ P @ alphas\n",
    "    # P = np.dot(cp.matmul(Y, X),cp.matmul(Y, X).T)\n",
    "    # P = cp.matmul(Y, Y.T) * X @ X.T\n",
    "    # P = cp.Constant(P)\n",
    "    quadratic_part = 0.5 * cp.sum_squares(cp.multiply(alphas, Y) @ X)\n",
    "    obj = cp.Maximize(cp.sum(alphas) - quadratic_part)\n",
    "   \n",
    "    # # # constraints: SVM dual form\n",
    "    cons = [\n",
    "        alphas >= 0,\n",
    "        alphas <= regularisation_para_c / n_samples,\n",
    "        cp.sum(cp.multiply(alphas, label_train)) == 0\n",
    "    ]\n",
    "    \n",
    "    # # # Form and solve problem\n",
    "    prob = cp.Problem(obj, cons)\n",
    "    prob.solve(solver=cp.SCS,verbose=True) # Returns the optimal value\n",
    "    \n",
    "    # # print the problem info - remove it after\n",
    "    # print(\"status:\", prob.status)\n",
    "    # print(\"optimal value\", prob.value)\n",
    "    \n",
    "    # # Extract the values - a numpy ndarray\n",
    "    alphas_value = alphas.value   \n",
    "\n",
    "    # # return svm dual model alphas\n",
    "    return alphas_value\n",
    "    # return obj\n",
    "\n",
    "# train dual model\n",
    "c = 100\n",
    "alphas = svm_train_dual(X_train, Y_train, c)\n",
    "print(alphas)\n",
    "# output svm dual form solutions\n",
    "print('Please copy the folowing result to Question 2 \"Sum of alphas = )\"')\n",
    "print(np.round(np.sum(alphas),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aff66d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the folowing result to Question 2 \"sum(W) = \"\n",
      "0.79\n",
      "Please copy the folowing result to Question 2 \"b = \"\n",
      "1.73\n",
      "Please copy the folowing result to Question 2 \"accuracy = \"\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "# obtain primal w*, b* from dual solution\n",
    "def find_model_params_from_dual(data, label, alphas, regularisation_para_c):\n",
    "    \n",
    "    # this value is used to compare values generated by CVXPYY to zero.\n",
    "    # for example, instead of \n",
    "    # if var > 0, we use: if var > zero_threshold\n",
    "    zero_threshold = 0.0001\n",
    "    w_dual = 0\n",
    "    b_dual = 0\n",
    "\n",
    "    # b is scalar , but may be sliglty diffrent for each Y, calculate b for each label and return mean of them\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "# Note: b_dual is a scalar, but maybe different for each label, therefore calculate the mean\n",
    "\n",
    "    # Compute w as a weighted sum of the data points\n",
    "    # print(\"alphas shape\", alphas.shape)\n",
    "    # print(\"label shape\", label.shape)\n",
    "    # print(\"data shape\", data.shape)\n",
    "    w_dual = (alphas * label) @ data\n",
    "    \n",
    "    # Identify support vectors (those with 0 < alpha < C)\n",
    "    sv_idx = (alphas > zero_threshold) & (alphas < regularisation_para_c - zero_threshold)\n",
    "    \n",
    "    # Compute b using support vectors\n",
    "    support_vector_data = data[sv_idx]\n",
    "    support_vector_labels = label[sv_idx]\n",
    "    b_dual_values = support_vector_labels - support_vector_data @ w_dual\n",
    "    b_dual = np.mean(b_dual_values)\n",
    "    \n",
    "# =========================================================================\n",
    "    # print(\"sv_idx: \",sv_idx)\n",
    "    # print(\"w_dual: \",w_dual)\n",
    "    # print(\"b_dual: \",b_dual)\n",
    "    return w_dual, b_dual\n",
    "\n",
    "# output reconstructed w* and b* from svm dual problem\n",
    "c = 100\n",
    "model_dual = find_model_params_from_dual(X_train, Y_train, alphas, c)\n",
    "\n",
    "# predict and output accuracy of dual model\n",
    "accuracy = svm_predict(X_test, Y_test, model_dual)\n",
    "print('Please copy the folowing result to Question 2 \"sum(W) = \"')\n",
    "print(np.round(np.sum(model_dual[0]),2))\n",
    "print('Please copy the folowing result to Question 2 \"b = \"')\n",
    "print(np.round(model_dual[1],2))\n",
    "print('Please copy the folowing result to Question 2 \"accuracy = \"')\n",
    "print(np.round(accuracy,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b83ea70-ae9e-4baf-8835-6833f177da60",
   "metadata": {},
   "source": [
    "## Question 3: PCA\n",
    "\n",
    "1. Perform PCA on the dataset to reduce each sample into a 10-dimensional feature vector.\n",
    "2. For easy verification, show the sum of covariance matrix.\n",
    "\n",
    "=========================================================================\n",
    "- Implementing PCA algorithm.\n",
    "    - Start\n",
    "        - Input: n no. of samples as matrix $X$ of $n$ rows and $k$ columns.\n",
    "        - Calculate the mean for each column. $$mean = \\frac {1}{n} \\sum \\limits _{i=1} ^{n}X_{ij}$$\n",
    "        - Calculate the centralised matrix $X_C$ and covariance matrix $C$. $$X_C=X-mean$$ $$C = \\frac {1}{n}(X_C)^TX_C$$\n",
    "        - Calculate the eigenvalues and eigenvectors using convariance matrix.\n",
    "        - Select top x principal components - which are eigen vector corresponding to top x eigen values. Construct matrix $P$.\n",
    "    - End\n",
    "    \n",
    "- Transforming the the data using the principal components (matrix $P$) obtained using the PCA algorithm. $$Transformed \\: Data = XP$$\n",
    "- Calculating the covariance matrix of the transformed data by first centralising it(mean subtracted) and then obtaining the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32c40204",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtfdriY9Fhyw",
    "outputId": "adc09825-1942-4209-ab34-e4d6148a3d8e"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aed4563b",
   "metadata": {
    "id": "huRMLmS2IQjT"
   },
   "outputs": [],
   "source": [
    "# get training dataset\n",
    "train = \"train.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X = df[:2000].iloc[:, 1:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55475a58-cd32-4c7f-8263-ffad66c91dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the folowing result to Question 3 \"Cov X = )\"\n",
      "263.04\n",
      "Please copy the folowing result to Question 3 \"Cov X_transformed = )\"\n",
      "57.0\n"
     ]
    }
   ],
   "source": [
    "# Selecting top 10 Principal components\n",
    "no_of_components = 10\n",
    "\n",
    "covariance_matrix_X = 0\n",
    "covariance_matrix_X_transformed = 0\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "def PCA(X, num_component):\n",
    "    \n",
    "    ## origin dataset\n",
    "    # Input\n",
    "    \n",
    "    # Center the data: Calculate the mean of each column then subtracting the mean\n",
    "    x_center = X - np.mean(X, axis = 0)\n",
    "    \n",
    "    # Calculate the covariance matrix\n",
    "    x_cov = x_center.T @ x_center / x_center.shape[0]\n",
    "    \n",
    "    # Calculate the eigenvalues and eigenvectors using convariance matrix.\n",
    "    ei_value, ei_vec = np.linalg.eigh(x_cov)\n",
    "    \n",
    "    # Sort the eigenvectors based on the eigenvalues\n",
    "    idx = np.argsort(ei_value)[::-1]\n",
    "    ei_vec = ei_vec[:,idx]\n",
    "    \n",
    "    # Select top x principal components\n",
    "    top_p_components = ei_vec[:,:num_component]\n",
    "    \n",
    "    ## tranformed dataset\n",
    "    # Input\n",
    "    x_tran = X @ top_p_components\n",
    "    \n",
    "    # Center the data: Calculate the mean of each column then subtracting the mean\n",
    "    x_tran_center = x_tran - np.mean(x_tran, axis = 0)\n",
    "    \n",
    "    # Calculate the covariance matrix\n",
    "    x_tran_cov = x_tran_center.T @ x_tran_center / x_tran_center.shape[0]\n",
    "    \n",
    "    return [x_cov, x_tran_cov]\n",
    "    \n",
    "# Get the covariance matrix both before and after tranformation\n",
    "covariance_matrix_X, covariance_matrix_X_transformed = PCA(X, no_of_components)\n",
    "\n",
    "# ==========================================================================\n",
    "\n",
    "sum_cov_X = np.sum(covariance_matrix_X)\n",
    "sum_cov_X_transformed = np.sum(covariance_matrix_X_transformed)\n",
    "\n",
    "print('Please copy the folowing result to Question 3 \"Cov X = )\"')\n",
    "print(np.round(np.sum(sum_cov_X),2))\n",
    "print('Please copy the folowing result to Question 3 \"Cov X_transformed = )\"')\n",
    "print(np.round(np.round(sum_cov_X_transformed,2)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a331268",
   "metadata": {},
   "source": [
    "## Question 4: Kernel k-Means\n",
    "\n",
    "In this question you will implement kernel k-means with RBF-kernel, using SpectralClustering from sklearn to reduce the programming efford of the full implementation. \n",
    "Please implement kernel k-means algorithm with RBF-kernel, that is:\n",
    "\n",
    "$$(\\mathbf{x}_i,\\mathbf{x}_j) = \\exp(\\frac{-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2}{2\\sigma^2})$$\n",
    "\n",
    "For the $2\\sigma^2$ parameter, instead of a constant value, please use mean pairwise squared distance between the datapoints, that is:\n",
    "\n",
    "$$2\\sigma^2 = \\frac{1}{N^2} \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\| \\mathbf{x}_i -\\mathbf{x}_j\\|_2^2$$ \n",
    "\n",
    "In order to calculate paiwise distance matrix, please use scipy.spatial distance function, which is alread pre-loaded in the code. <br>\n",
    "Please check the documentation of the SpectralClustering function to use precomputed arrinity matrix, which you are going to supply with the above specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "309b239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from scipy.spatial import distance\n",
    "\n",
    "df = pd.read_csv('train.csv', header = None)\n",
    "X = df.iloc[:1000,1:].to_numpy(copy=True)\n",
    "Y = df.iloc[:1000,:1].to_numpy(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "494c1d54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the folowing result to Question 4 \"Sum of centroids = )\"\n",
      "36.78\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 10\n",
    "centroids_rbf = np.array(n_clusters)\n",
    "\n",
    "centroids_rbf = []\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "# use random_state=0 in the SpectralClustering function to make the results reproducible.\n",
    "\n",
    "# calculate pairwise distance\n",
    "pw_sq_dist = distance.squareform(distance.pdist(X,'sqeuclidean'))\n",
    "\n",
    "# mean pairwise - 2lambda\n",
    "mean_pw_sq_dist = 0.5 * np.sum(pw_sq_dist) / (X.shape[0]**2)\n",
    "\n",
    "# calculate RBF kernal\n",
    "arrinity_matrix = np.exp(-pw_sq_dist/(mean_pw_sq_dist * 2))\n",
    "\n",
    "# construct the Spectral Clustering\n",
    "cluster = SpectralClustering(n_clusters = n_clusters, affinity='precomputed', random_state=0)\n",
    "label = cluster.fit_predict(arrinity_matrix)\n",
    "\n",
    "# find the centriod\n",
    "centroids_rbf = [np.mean(X[label == i], axis=0) for i in range(n_clusters)]\n",
    "centroids_rbf = np.array(centroids_rbf)\n",
    "\n",
    "# =================================================================\n",
    "    \n",
    "print('Please copy the folowing result to Question 4 \"Sum of centroids = )\"')\n",
    "print(np.round(np.sum(centroids_rbf),2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11caf2e7",
   "metadata": {},
   "source": [
    "## Question 5: Adaboost questions\n",
    "\n",
    "Answer questions in myUni assignment 2\n",
    "\n",
    "- [FALSE] Using Adaboost with a stronger weak classifier, for example, weak classifiers that can achieve higher accuracy when they are used individually, can always lead to better accuracy on the test set.\n",
    "- [TRUE] Assume that the weak learners are a finite set of decision stumps, Adaboost cannot achieve zero training error if the training data is not linearly separable.\n",
    "- [TRUE] Assume that the weak learners are a finite set of decision stumps, normalising features (scaling each dimension of features to ensure that each dimension has zero mean and unit variance) will not impact the predictive accuracy on the test set.\n",
    "- [FALSE] Once a weak classifier is picked in a particular round, it will never be chosen in any subsequent round."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05d6f7e9",
   "metadata": {},
   "source": [
    "## Question 6: Adaboost code correction\n",
    "The following code shows an incomplete implementation of Adaboost algorithm. The code does not implement the logic specific to Adaboost. Please modify it to make a correct implementation. Modify the code only in sections as indicated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7b02fef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtfdriY9Fhyw",
    "outputId": "adc09825-1942-4209-ab34-e4d6148a3d8e"
   },
   "outputs": [],
   "source": [
    "##### do not change this code\n",
    "\n",
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from math import exp, log as ln\n",
    "\n",
    "number_of_models = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ed01d5b",
   "metadata": {
    "id": "huRMLmS2IQjT"
   },
   "outputs": [],
   "source": [
    "##### do not change this code\n",
    "\n",
    "# get training dataset\n",
    "train = \"train.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X_train = df.iloc[:, 1:].to_numpy()\n",
    "Y_train = df.iloc[:, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e9fcdd8",
   "metadata": {
    "id": "4G3CJIcaIogN"
   },
   "outputs": [],
   "source": [
    "##### do not change this code\n",
    "\n",
    "# get test dataset\n",
    "test = \"test.csv\"\n",
    "df = pd.read_csv(test, header=None)\n",
    "X_test = df.iloc[:, 1:].to_numpy()\n",
    "Y_test = df.iloc[:, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8f91527",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### do not change this code\n",
    "\n",
    "def weak_classifier_train(train_data, train_label, sample_weights):\n",
    "    # Create a decision stump\n",
    "    stump = DecisionTreeClassifier(max_depth=1)\n",
    "    \n",
    "    # Train the stump using the weighted samples\n",
    "    stump.fit(train_data, train_label, sample_weight=sample_weights)\n",
    "    \n",
    "    # Generate predictions and calculate error\n",
    "    model_prediction = stump.predict(train_data)\n",
    "    train_label = np.array(train_label)\n",
    "    model_prediction = np.array(model_prediction)\n",
    "    sample_weights = np.array(sample_weights)\n",
    "\n",
    "    # The following line should work if both train_label and model_prediction are numpy arrays of the same size.\n",
    "    error = np.sum(sample_weights[train_label != model_prediction])\n",
    "    \n",
    "    return stump, error, model_prediction\n",
    "\n",
    "##### do not change this code\n",
    "\n",
    "def weak_classifier_prediction(test_data, model):\n",
    "    \n",
    "    # The model_param_t is our trained stump, so we use it to make predictions\n",
    "    return model.predict([test_data])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51abec79",
   "metadata": {},
   "source": [
    "**You can change the code in the following section only**\n",
    "**Please clearly comment the purpose of the code that you added or changed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa89a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adaboost_train(train_data, train_label, T):\n",
    "    # train_data: N x d matrix\n",
    "    # train_label: N x 1 vector\n",
    "    # T: the number of weak classifiers in the ensemble\n",
    "\n",
    "    ensemble_models = []\n",
    "    alpha_value = [] ## [Adding] - check list to collet alpha value\n",
    "\n",
    "    N = len(train_label)  # The number of samples\n",
    "\n",
    "    # Initialize weight distribution for each sample to be evenly distributed\n",
    "    D = np.array([1 / N] * N)\n",
    "\n",
    "    # For each weak classifier, do the following\n",
    "    for t in range(T):\n",
    "\n",
    "        # weak_classifier_train takes the training data and labels as well as the sample weight distribution\n",
    "        # returns the model parameters and the error\n",
    "        model, error, model_prediction = weak_classifier_train(train_data, train_label, D) \n",
    "        \n",
    "        ## [Adding] - calculate alpha from error\n",
    "        alpha = 0.5 * np.log((1 - error) / error)\n",
    "        \n",
    "        ## [Adding] - update new weight + normalization\n",
    "        D = D * np.exp(-alpha * train_label * model_prediction) / np.sum(D)\n",
    "        \n",
    "        # definition of model\n",
    "        ensemble_models.append(model)\n",
    "        alpha_value.append(alpha) ## [Adding] - append list to collet alpha value\n",
    "\n",
    "    return ensemble_models, alpha_value\n",
    "\n",
    "\n",
    "def Adaboost_test(test_data, ensemble_models, alpha_value): ## [Adding] alpha value to adjust the H = sum (alpha x pred)\n",
    "    # test_data: n x d\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        decision_ensemble = 0\n",
    "\n",
    "        # For each model in the ensemble models, do the following\n",
    "        for k in range(len(ensemble_models)):\n",
    "\n",
    "            # Make a prediction on the classification of the sample\n",
    "            # prediction returns 1 or -1 prediction from the weak classifier\n",
    "            prediction = weak_classifier_prediction(test_data[i], ensemble_models[k])\n",
    "\n",
    "            # Add this classification \n",
    "            decision_ensemble += alpha_value[k] * prediction ## [Adding] alpha value\n",
    "\n",
    "        # If more models predicted it as +1 class\n",
    "        if decision_ensemble > 0:\n",
    "            prediction = 1\n",
    "        # Else, more models predicted it as -1 class\n",
    "        else:\n",
    "            prediction = -1\n",
    "\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# predict and output accuracy\n",
    "\n",
    "ensemble_models, alpha_value = Adaboost_train(X_train, Y_train, number_of_models)\n",
    "predicted_labels = Adaboost_test(X_test, ensemble_models, alpha_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "338707dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please copy the following result line to Question 6 \"Accuracy = )\"\n",
      "0.981\n"
     ]
    }
   ],
   "source": [
    "######## do not change this code ############\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, predicted_labels)\n",
    "\n",
    "print('Please copy the following result line to Question 6 \"Accuracy = )\"')\n",
    "print(np.round(accuracy,3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1eb80bdb",
   "metadata": {},
   "source": [
    "## Question 7: Deep Learning\n",
    "\n",
    "Answer this question in myUni assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa34e61",
   "metadata": {},
   "source": [
    "## derive the gradient for tanh activation function\n",
    "\n",
    "01 Show the derivation following the function below\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial{E( {w})}}{\\partial{ {w}}} & = \\frac{\\partial{(\\frac{1}{2}\\sum(t - o)^{2})}}{\\partial{ {w}}} \\\\\n",
    "& = \\frac{\\partial{(\\frac{1}{2}\\sum(t - \\tanh(xw))^{2})}}{\\partial{ {w}}} \\\\\n",
    "& = \\frac{1}{2} * \\sum(t - \\tanh(xw)) * 2 * \\frac{\\partial{ (t - \\tanh(xw))}}{\\partial{ {w}}}\\\\\n",
    "& = - \\sum (t - \\tanh(xw)) \\frac{\\partial{\\tanh(xw)}}{\\partial{ {w}}} \\\\\n",
    "& = - \\sum (t - \\tanh(xw)) (1 - \\tanh^{2}(xw)) \\frac{\\partial{(xw)}}{\\partial{ {w}}} \\\\\n",
    "& = - \\sum (t - \\tanh(xw)) (1 - \\tanh^{2}(xw)) x \\\\\n",
    "\\end{align*}\n",
    "\n",
    "given just one example of the following data \n",
    "\\begin{align*}\n",
    "x & = 1.0 \\\\\n",
    "w & = 0.3 \\\\\n",
    "t & = 1\n",
    "\\end{align*}\n",
    "\n",
    "02 Calculate the output of the activation function\n",
    "\n",
    "\\begin{align*}\n",
    "f(xw) & = x * W \\\\\n",
    "& = 1.0 * 0.3 \\\\\n",
    "& = 0.3 \\\\\n",
    "output & = tanh(xw) \\\\\n",
    "& = \\frac{1 - e^{-2x}}{1 + e^{-2x}} \\\\\n",
    "& = \\frac{1 - e^{-2 * 0.3}}{1 + e^{-2 * 0.3}} \\\\\n",
    "& = \\frac{1 - e^{-0.6}}{1 + e^{-0.6}} \\\\\n",
    "& = 0.2913 \\approx 0.29\n",
    "\\end{align*}\n",
    "\n",
    "03 Calculate the gradient value\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial{E( {w})}}{\\partial{ {w}}} & = - \\sum (t - \\tanh(xw)) (1 - \\tanh^{2}(xw)) x \\\\\n",
    "& = -(1 - 0.29)) (1 - 0.29^{2}) * 1.0 \\\\\n",
    "& = -0.6502 \\approx -0.65\n",
    "\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
